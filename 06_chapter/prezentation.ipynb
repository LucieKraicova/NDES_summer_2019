{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6. Kernel Smoothing Methods\n",
    "\n",
    "- **Local Regression** \n",
    "    - 6.1 Local Regression in $\\mathbb{R}^1$\n",
    "    - 6.3 Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.4 Structured Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.5 Local Likelyhood and Other Models\n",
    "- 6.6 **Kernel Density Estimation and Classification**\n",
    "- 6.7 Radial Basis Functions and Kernels\n",
    "- 6.8 Mixture Models for Density Estimation and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### What is a kernel? \n",
    "- weighting function $K_\\lambda(x_0,x_i)$, which assigns a weight to $x_i$ based on its distance from $x_0$.\n",
    "\n",
    "### How can we use kernels?\n",
    "- regression\n",
    "    - vizualization\n",
    "    - device for detecting nonlinearities\n",
    "    - localization device\n",
    "- classification\n",
    "- density estimation\n",
    "- ML \"kernel methods\" - kernel trick - regularized nonlinear modeling\n",
    "\n",
    "### What are advantages and disadvanteges? \n",
    "- memory-based - no training, all fitting during prediction \n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Local regression\n",
    "\n",
    "### How does local regression fit into the spectrum of regression methods?\n",
    "- **general regression setting**: \n",
    "    - estimating the regression function $f(X)$ over the domain $\\mathbb{R}^p$\n",
    "    - complexity restriction: \n",
    "        - define neighbourhood - explicitely controlled by widnow width and kernel type \n",
    "        - define behavior on that neighbourhood - constant, linear, polynomial \n",
    "- **model**: training data set, different at each point $x_0$.\n",
    "- **parameters** estimated from data: $\\lambda$\n",
    "- **estimated function** $\\hat{f}(X)$ is smooth in $\\mathbb{R}^p$\n",
    "- local regression estimate of $f(x_0)$ as $f_{\\hat{\\theta}}(x_0)$, where $\\hat{\\theta}$ minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of kernels\n",
    "\n",
    "### By type of window\n",
    "- **Metric window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D \\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "- **Adaptive window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{h_\\lambda(x_0)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "    - K nearest neighbours \n",
    "\\begin{equation}\n",
    "K_k(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{\\|x-x_k\\|} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Window width parameter:\n",
    "- the radius of the support region $\\lambda$\n",
    "- standard deviation for Gaussian kernel\n",
    "-  number $k$ of nearest neighbors in $k$-nearest neighborhoods\n",
    "\n",
    "## By functional form\n",
    "- **Compact support**\n",
    "- Epanechnikov\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac34 (1-t^2) & \\text{if } |t| \\le 1; \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    " - Tri-cube\n",
    "    \n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{70}{81}(1-|t|^3)^3 & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "- Uniform\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{1}{2} & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- **Infinite support** \n",
    "    - Gaussian \n",
    "\\begin{equation}    \n",
    "K_\\lambda(x_0,x) = \\frac{1}{\\lambda}\\exp\\left(-\\frac{\\|x-x_0\\|^2}{2\\lambda}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/kernels_textbook.png) \n",
    "\n",
    "\n",
    "## Questions: \n",
    "- Can we combine any kernel functional form with any type of window? \n",
    "- Why do we care about the type of window (metric vs. adaptive)? (How does the type of window influence prediction in case of gaps in data, boundaries, sparse areas?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the polynomial order\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "K nearest neighbours\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\text{Ave}\\left( y_i|x_i\\in N_k(x_0) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Nadarayea-Watson weighted average\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Local linear regression\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\beta(x_0)x_i \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Local polynomial regression\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta_j(x_0), j=1,\\cdots,d} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\sum_{j=1}^d \\beta_j(x_0)x_i^j \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\sum_{j=1}^N \\hat\\beta(x_0)x_o^j.\n",
    "\\end{equation}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Bias-variance tradeoff\n",
    "\\begin{align}\n",
    "\\text{E}\\hat{f}(x_0) &= \\sum_{i=1}^N l_i(x_0)f(x_i) \\\\\n",
    "&= f(x_0)\\sum_{i=1}^N l_i(x_0) + f'(x_0)\\sum_{i=1}^N (x_i-x_0)l_i(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R,\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat{f}(x_0)) = \\sigma^2 \\|l(x_0)\\|^2,\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/constant_to_linear.png) \n",
    "![title](imgs/linear_to_quadratic.png) \n",
    "![title](imgs/variance_and_polynomial_order.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting window width\n",
    "\n",
    "Bias-variance tradeoff: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#CONSTANTS\n",
    "random.seed(42)\n",
    "size_sample = 100\n",
    "xgrid = scipy.linspace(0, 1, 1001)\n",
    "y_true = scipy.sin(4*xgrid)\n",
    "x_sample = scipy.random.uniform(size=size_sample)\n",
    "y_sample = scipy.sin(4*x_sample) + scipy.randn(size_sample)/3\n",
    "\n",
    "# K-nearest neighbors \n",
    "def knn(k: int, point:float,\n",
    "        data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    idx_sorted = scipy.argsort((data_x-point)*(data_x-point))[:k]\n",
    "    return data_y[idx_sorted].mean()\n",
    "\n",
    "# Constant fit with Epanechnikov\n",
    "def epanechnikov(lmbda:float, point:float,\n",
    "                 data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    t = scipy.absolute(data_x-point)/lmbda  # argment for D\n",
    "    k = scipy.where(t <= 1, 0.75*(1-t), 0)\n",
    "    return (k @ data_y).sum()/k.sum()\n",
    "\n",
    "\n",
    "# Constant fit with gaussian\n",
    "def gaussian(lmbda:float, point:float,\n",
    "                 data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    k = (1/lmbda) * scipy.exp(\n",
    "        -(scipy.absolute(data_x-point)*scipy.absolute(data_x-point)/(2*lmbda))) \n",
    "    return (k @ data_y).sum()/k.sum()\n",
    "\n",
    "# Constant fit with tricube\n",
    "def tricube(lmbda:float, point:float,\n",
    "                 data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    t = scipy.absolute(data_x-point)/lmbda  # argment for D\n",
    "    k = scipy.where(t <= 1, (70/81)*np.power((1-np.power(scipy.absolute(t),3)),3), 0)\n",
    "    return (k @ data_y).sum()/k.sum()\n",
    "\n",
    "def plot_knn(ax,k,idx_x0,show_true_y,show_estimated_y):\n",
    "        x0 = xgrid[idx_x0]  # .5\n",
    "        x_k = x_sample[scipy.argsort((x_sample-x0)*(x_sample-x0))[k-1:k]][0]\n",
    "        mask = scipy.absolute(x_sample-x0) <= scipy.absolute(x0-x_k)\n",
    "        set_ax(ax=ax, \n",
    "               y_estimated=scipy.array([knn(k, x, x_sample, y_sample) for x in xgrid]), \n",
    "               mask=mask, \n",
    "               title='Nearest-Neighbor Kernel',\n",
    "               idx_x0=idx_x0, \n",
    "               show_true_y=show_true_y,show_estimated_y=show_estimated_y)\n",
    "\n",
    "def plot_epanechnikov(ax,lmbda,idx_x0,show_true_y,show_estimated_y):\n",
    "    x0 = xgrid[idx_x0]  # .5\n",
    "    set_ax(ax=ax, \n",
    "           y_estimated = scipy.array([epanechnikov(lmbda, x, x_sample, y_sample)\n",
    "                                  for x in xgrid]), \n",
    "           mask = scipy.absolute(x_sample-x0)/lmbda <= 1, \n",
    "           title='Epanechnikov Kernel',\n",
    "           idx_x0=idx_x0,\n",
    "           show_true_y=show_true_y,show_estimated_y=show_estimated_y) \n",
    "\n",
    "def plot_gaussian(ax,lmbda,idx_x0,show_true_y,show_estimated_y):\n",
    "    x0 = xgrid[idx_x0]\n",
    "    set_ax(ax=ax, \n",
    "           y_estimated = scipy.array([gaussian(lmbda, x, x_sample, y_sample)\n",
    "                                  for x in xgrid]), \n",
    "           mask = np.array([True]*len(x_sample)),\n",
    "           title='Gaussian Kernel',\n",
    "           idx_x0=idx_x0,\n",
    "           show_true_y=show_true_y,show_estimated_y=show_estimated_y) \n",
    "\n",
    "def plot_tricube(ax,lmbda,idx_x0,show_true_y,show_estimated_y):\n",
    "    x0 = xgrid[idx_x0]\n",
    "    set_ax(ax=ax, \n",
    "           y_estimated = scipy.array([tricube(lmbda, x, x_sample, y_sample)\n",
    "                                  for x in xgrid]), \n",
    "           mask = scipy.absolute(x_sample-x0)/lmbda <= 1,\n",
    "           title='Tricube Kernel',\n",
    "           idx_x0=idx_x0,\n",
    "           show_true_y=show_true_y, show_estimated_y=show_estimated_y) \n",
    "\n",
    "def set_ax(ax, y_estimated, mask, title:str, idx_x0=500, show_true_y=True,show_estimated_y=True):\n",
    "    x0 = xgrid[idx_x0]  # .5\n",
    "    if show_true_y:\n",
    "        ax.plot(xgrid, y_true, color='C0', linewidth=2)\n",
    "    if show_estimated_y:\n",
    "        ax.plot(xgrid, y_estimated, color='C2')\n",
    "        ax.plot((x0, x0), (min(y_sample), y_estimated[idx_x0]), 'o-', color='C3')\n",
    "        ax.set_title(title)\n",
    "    ax.plot(x_sample[mask], y_sample[mask],'o', color='C3', mfc='none')\n",
    "    ax.plot(x_sample[~mask], y_sample[~mask],'o', color='C0', mfc='none')\n",
    "   \n",
    "    \n",
    "def plot_two(right:str, left:str, \n",
    "             k_left:int, k_right:int,lmbda_left:float,lmbda_right:float,\n",
    "             idx_x0=500, show_true_y=True,show_estimated_y=True):\n",
    "    fig61 = plt.figure(61, figsize=(18, 10))\n",
    "    ax1=fig61.add_subplot(1, 2, 1)\n",
    "    ax2=fig61.add_subplot(1, 2, 2)\n",
    "    \n",
    "    if left == 'tricube':\n",
    "        plot_tricube(ax1,lmbda_left,idx_x0,show_true_y, show_estimated_y)\n",
    "    elif left == 'epanechnikov':\n",
    "        plot_epanechnikov(ax1,lmbda_left,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif left == 'gaussian':\n",
    "        plot_gaussian(ax1,lmbda_left,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif left == 'knn':\n",
    "        plot_knn(ax1,k_left,idx_x0,show_true_y,show_estimated_y) \n",
    "    else: \n",
    "        raise ValueError('Unsupported left kernel name %s' %left)\n",
    "        \n",
    "    if right == 'tricube':\n",
    "        plot_tricube(ax2,lmbda_right,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif right == 'epanechnikov':\n",
    "        plot_epanechnikov(ax2,lmbda_right,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif right == 'gaussian':\n",
    "        plot_gaussian(ax2,lmbda_right,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif right == 'knn':\n",
    "        plot_knn(ax2,k_right,idx_x0,show_true_y,show_estimated_y) \n",
    "    else: \n",
    "        raise ValueError('Unsupported right kernel name %s' %right)\n",
    "\n",
    "def plot_one(kernel:str, \n",
    "     k:int,lmbda,\n",
    "     idx_x0=500, show_true_y=True,show_estimated_y=True):\n",
    "    fig61 = plt.figure(61, figsize=(18, 10))\n",
    "    ax=fig61.add_subplot(1, 2, 1)\n",
    "    \n",
    "    if kernel == 'tricube':\n",
    "        plot_tricube(ax,lmbda,idx_x0,show_true_y, show_estimated_y)\n",
    "    elif kernel == 'epanechnikov':\n",
    "        plot_epanechnikov(ax,lmbda,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif kernel == 'gaussian':\n",
    "        plot_gaussian(ax,lmbda,idx_x0,show_true_y,show_estimated_y)\n",
    "    elif kernel == 'knn':\n",
    "        plot_knn(ax,k,idx_x0,show_true_y,show_estimated_y) \n",
    "    else: \n",
    "        raise ValueError('Unsupported kernel name %s' %kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b707b1ac0a59434687d4c5e670f1885b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='right', options=('tricube', 'epanechnikov', 'gaussian', 'knn'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_two(right: str, left: str, k_left: int, k_right: int, lmbda_left: float, lmbda_right: float, idx_x0=500, show_true_y=True, show_estimated_y=True)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_two, \n",
    "         right = ['tricube','epanechnikov','gaussian','knn'],\n",
    "         left = ['tricube','epanechnikov','gaussian','knn'],\n",
    "         k_left=widgets.IntSlider(min=1,max=100,step=1,value=10),\n",
    "         k_right=widgets.IntSlider(min=1,max=100,step=1,value=10),\n",
    "         lmbda_left=(0.02,1.0,0.01),\n",
    "         lmbda_right=(0.02,1.0,0.01),\n",
    "         idx_x0=widgets.IntSlider(min=1,max=1000,step=1,value=500),\n",
    "         show_true_y=False,\n",
    "        show_estimated_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb9dd1715f04f54a052a60bbb751dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='kernel', options=('tricube', 'epanechnikov', 'gaussian', 'knn'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_one(kernel: str, k: int, lmbda, idx_x0=500, show_true_y=True, show_estimated_y=True)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_one, \n",
    "         kernel = ['tricube','epanechnikov','gaussian','knn'],\n",
    "         k=widgets.IntSlider(min=1,max=100,step=1,value=10),\n",
    "         lmbda=(0.02,1.0,0.01),\n",
    "         idx_x0=widgets.IntSlider(min=1,max=1000,step=1,value=500),\n",
    "         show_true_y=False,\n",
    "        show_estimated_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.3. Local Regression in $\\mathbb{R}^p$\n",
    "### Generalizing previous notation\n",
    "\n",
    "Let $b(X)$ be a vector of polynomial terms in $X$ of maximum degree $d$; e.g., with $d=1$ and $p=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2);\n",
    "\\end{equation}\n",
    "\n",
    "with $d=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2);\n",
    "\\end{equation}\n",
    "\n",
    "and trivially with $d=0$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "At each $x_0 \\in \\mathbb{R}^p$ solve\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - b(x_i)^T\\beta(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "to produce the fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = b(x_0)^T \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Typically the kernel will be a radial function, such as the radial Epanechnikov or tri-cube kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|\\cdot\\|$ is the Euclidean norm.\n",
    "\n",
    "Since the Euclidean norm depends on the units in each coordinate, it makes most sense to standardize each predictor, e.g., to unit standard deviation, prior to smoothing.\n",
    "\n",
    "### Problems with high dimension \n",
    "\n",
    "- **Curse of dimensionality** \n",
    "    - impossible to simultaneously maintain localness ($\\Rightarrow$ low bias) and a sizeable sample in the neighborhood ($\\Rightarrow$ low variance) as the dimension increases, without the total sample size increasing exponentially in $p$.\n",
    "    - correcting by local or higher polynomial fit is useful\n",
    "- **Vizualization problematic**\n",
    "\n",
    "\n",
    "- Solution: introduce structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.4. Structured Local Regression Models in $\\mathbb{R}^p$\n",
    "\n",
    "> When the dimension to sample-size ratio is unfavorable, local regression does not help us much, unless we are willing to make some structural assumptions about the model.\n",
    ">\n",
    "> Much of this book is about structured regression and classification models. Here we focus on some approaches directly related to kernel methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.1. Structured Kernels\n",
    "### Standardization\n",
    "\n",
    "One line of approach is to modify the kernel. The default spherical kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "gives equal weight to each coordinate, and so a natural default strategy is to standardize each variable to unit standard deviation.\n",
    "\n",
    "A more general approach is to use a positive semidefinite matrix $\\mathbf{A}$ to weigh the different coordinates:\n",
    "\n",
    "\\begin{equation}\n",
    "K_{\\lambda,\\mathbf{A}}(x_0,x) = D \\left( \\frac{(x-x_0)^T\\mathbf{A}(x-x_0)}\\lambda \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on $\\mathbf{A}$. For example, if $\\mathbf{A}$ is diagonal, then we can increase or decrease the influence of individual predictors $X_j$ by increasing or decreasing $A_{jj}$.\n",
    "\n",
    "Often the predictors are many and highly correlated, such as those arising from digitized analog signals or images. The covariance function of the predictors can be used to tailor a metric $\\mathbf{A}$ that focuses less, say, on high-freqeuncy contrast (Exercise 6.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.2. Structured Regression Functions\n",
    "\n",
    "We are trying to fit a regression function\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}(Y|X) = f(X_1,X_2,\\cdots,X_p)\n",
    "\\end{equation}\n",
    "\n",
    "in $\\mathbb{R}^p$, in which every level of interaction is potentially present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure via ANOVA decomposition\n",
    "It is natural to consider ANOVA decompositions of the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(X_1,X_2,\\cdots,X_p) = \\alpha + \\sum_j g_j(X_j) + \\sum_{k<l} g_{kl}(X_k,X_l) + \\cdots\n",
    "\\end{equation}\n",
    "\n",
    "and then introduce structure by eliminating some of the higher-order terms.\n",
    "\n",
    "Additive models assume only main effect terms:\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha + \\sum_{j=1}^p g_j(X_j);\n",
    "\\end{equation}\n",
    "\n",
    "Second-order models will have terms with interactions of order at most two, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfitting\n",
    "In Chapter 9, we describe iterative _backfitting_ algorithms for fitting such lower-order interaction models. In the additive model, for example, if all but the $k$th term is assumed known, then we can estimate $g_k$ by local regression of $Y - \\sum_{j\\neq k}g_j(X_j)$ on $X_k$. This is done for each function in turn, repeatedly, until convergence. The important detail is that at any stage, 1D local regression is all that is needed. The same ideas can be used to fit low-dimensional ANOVA decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying coefficients models\n",
    "An important special case of these structured models are the class ofr _varying coefficient models_. Suppose that we divide the $p$ predictors in $X$ into a set $(X_1, X_2, \\cdots, X_q)$ with $q<p$, and the remainder of the variables we collect in the vector $Z$. We then assume the conditionally linear model\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha(Z) + \\beta_1(Z)X_1 + \\cdots + \\beta_q(Z)X_q.\n",
    "\\end{equation}\n",
    "\n",
    "For given $Z$, this is a linear model, but each of the coefficients can vary with $Z$. It is natural to fit such a model by locally weighted least squares:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(z_0),\\beta(z_0)} \\sum_{i=1}^N K_\\lambda(z_0,z_i) \\left( y_i - \\alpha(z_0) - x_{i1}\\beta_1(z_0) - \\cdots - x_{qi}\\beta_q(z_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 6.10 illustrates the idea on measurements of the human aorta. A longstanding claim has been that the aorta thickens with $\\textsf{age}$. Here we model the $\\textsf{diameter}$ of the aorta as a linear function of $\\textsf{age}$, but allow the coefficients to vary with $\\textsf{gender}$ and $\\textsf{depth}$ down the aorta. We used a local regression model separately for males and females. Wile aorta clearly does thicken with age at the higher regions of the aorta, the relationship fades with distance down the aorta. FIGURE 6.11 shows the intercept and slope as a function of depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.5. Local Likelihood and Other Models\n",
    "\n",
    "The concept of local regression and varying coefficient models is extremely broad: Any parametric model can be made local if the fitting method accommodates observation weights. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each observation $y_i$ is a parameter\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i = \\theta(x_i) = x_i^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "linear in the covariate(s) $x_i$, and inference for $\\beta$ is based on the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta) = \\sum_{i=1}^N l(y_i,x_i^T\\beta).\n",
    "\\end{equation}\n",
    "\n",
    "We can model $\\theta(X)$ more flexibly by using the likelihood local to $x_0$ for inference of $\\theta(x_0) = x_0^T\\beta(x_0)$:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta(x_0)) = \\sum_{i=1}^N K_\\lambda(x_0,x_i) l(y_i,x_i^T\\beta(x_0)).\n",
    "\\end{equation}\n",
    "\n",
    "Many likelihood models, in particular the family of  generalized linear models including logistic and log-linear models, involve the covariates in a linear fashion. Local likelihood allows a relaxation from a globally linear model to one that is locally linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of local likelihood, we consider the local version of the multiclass linear logistic regression model of Chapter 4. The data consist of features $x_i$ and an associated categorical response $g_i \\in \\{1,2,\\cdots,J\\}$, and the linear model has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=j|X=x) = \\frac{\\exp\\left(\\beta_{j0} + \\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\beta_{k0} + \\beta_k^Tx\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "The local log-likelihood for this $J$ class model can be written\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left\\{ \\beta_{g_i 0}(x_0) + \\beta_{g_i}^T(x_i-x_0) - \\log\\left( 1 + \\sum_{k=1}^{J-1} \\exp\\left( \\beta_{k0}(x_0) + \\beta_k(x_0)^T(x_i-x_0) \\right) \\right) \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Notice that\n",
    "* we have used $g_i$ as a subscript in the first line to pick out the appropriate numerator;\n",
    "* $\\beta_{J0} = 0$ and $\\beta_J = 0$ by the definition of the model;\n",
    "* we have centered the local regressions at $x_0$, so that the fitted posterior probabilities at $x_0$ are simply  \n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat{\\text{Pr}}(G=j|X=x) = \\frac{\\exp\\left(\\hat\\beta_{j0} + \\hat\\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\hat\\beta_{k0} + \\hat\\beta_k^Tx\\right)}.\n",
    "  \\end{equation}\n",
    "  \n",
    "This model can be used for flexible multiclass classification in moderately low dimensions, although successes have been reported with the high-dimensional ZIP-code classficiation problem. Generalized additive models (Chapter 9) using kernel smoothing methods are closely related, and avoid dimensionality problems by assuming an additive structure for the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.6. Kernel Density Estimation and Classification\n",
    "\n",
    "Kernel density estimation is an unsupervised learning procedure, which historically precredes kernel regression. It also leads to naturally to a simple family of procedures for nonparametric classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.1. Kernel Density Estimation\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "> Suppose we have a random sample $x_1,\\cdots,x_N$ drawn from a probability density $f_X(x)$, and we wish to estimate $f_X$ at a point $x_0$.\n",
    "\n",
    "For simplicity assume for now that $X\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local estimate\n",
    "\n",
    "A natural local estimate has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_X(x_0) = \\frac{\\#x_i \\in \\mathcal{N}(x_0)}{N\\lambda},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{N}$ is a small metric neighborhood around $x_0$ of width $\\lambda$.\n",
    "\n",
    "This estimate is bumpy, so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate with kernels\n",
    "\n",
    "the smooth _Parzen_ estimate is preferred.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N\\lambda} \\sum_{i=1}^N K_\\lambda(x_0,x_i),\n",
    "\\end{equation}\n",
    "\n",
    "because it counts observations close to $x_0$ with weights that decrease with distance from $x_0$. In this case a popular choice for $K_\\lambda$ is the Gaussian kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = \\phi\\left(\\frac{|x-x_0|}\\lambda\\right).\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 6.13 shows a Gaussian kernel density fit to the sample values for $\\textsf{systolic blood pressure}$ for the $\\textsf{CHD}$ group.\n",
    "\n",
    "Letting $\\phi_\\lambda$ denote the Gaussian density with mean zero and standard-deviation $\\lambda$, the Parzen estimate has the form\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}_X(x) &= \\frac1N \\sum_{i=1}^N \\phi_\\lambda(x-x_i) \\\\\n",
    "&= \\left( \\hat{F}\\star\\phi_\\lambda \\right)(x),\n",
    "\\end{align}\n",
    "\n",
    "the convolution of the sample empirical distribution $\\hat{F}$ with $\\phi_\\lambda$. The distribution $\\hat{F}(x)$ puts mass $1/N$ at each of the observed $x_i$, and is jumpy; in $\\hat{f}_X(x)$ we have smoothed $\\hat{F}$ by adding independent Gaussian noise to each observation $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution = local averaging\n",
    "\n",
    "The Parzen density estimate is the equivalent of the local average, and improvements have been proposed along the lines of local regression [on the log scale for densities; see Loader (1999)]. We will not pursue these here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In $\\mathbb{R}^p$\n",
    "\n",
    "The natural generalization of the Gaussian density estimate amounts to using the Gaussian product kernel,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N(2\\lambda^2\\pi)^{\\frac{p}2}} \\sum_{i=1}^N \\exp \\left\\{ -\\frac12 \\left( \\|x_i-x_0\\|/\\lambda \\right)^2 \\right\\}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.2. Kernel Density Classification\n",
    "\n",
    "One can use nonparametric density estimates for classification in a straight-forward fashion using Bayes' theorem.\n",
    "\n",
    "Suppose for a $J$ class problem\n",
    "* we fit nonparametric density estimates $\\hat{f}_j(X)$, for $j=1,\\cdots,J$ separately in each of the classes,\n",
    "* and we also have estimates of the class priors $\\hat\\pi_j$ (usually the sample proportions).\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{\\hat\\pi_j \\hat{f}_j(x_0)}{\\sum_{k=1}^J \\hat\\pi_k \\hat{f}_k(x_0)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty with sparse data\n",
    "\n",
    "FIGURE 6.14 uses this method to estimate to prevalence of $\\textsf{CHD}$ for the heart risk factor study, and should be compared with the left panel of FIGURE 6.12. The main difference occurs in the region of high SBP in the right panel of FIGURE 6.14. In this region the data are sparse for both classes, and since the Gaussian kernel density estimates use metric kernels, the density estimates are low and of poor quality (high variance) in these region.\n",
    "\n",
    "The local logistic regression method uses the tri-cube kernel with $k$-NN of the local linear assumption to smooth out the estimate (on the logit scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If classification is the ultimate goal, then learning the separate class densities well may be unnecessary, and can in fact be misleading.\n",
    "\n",
    "FIGURE 6.15 shows an example where the densities are both multimodal, but the posterior ratio is quite smooth. In learning the separate densities from data, one might decide to settle for a rougher, high-variance fit to capture these features, which are irrelevant for the purposes of estimating the posterior probabilities. In fact, if classification is the ultimate goal, then we need only to estimate the posterior well near the decision boundary. e.g., for two classes, this is the set\n",
    "\n",
    "\\begin{equation}\n",
    "\\{ x| \\text{Pr}(G=1|X=x) = \\frac12 \\}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.3. The Naive Bayes Classifier\n",
    "\n",
    "This is a technique that has remained popular over the years, despite its name (a.k.a. \"Idiot's Bayes\"). It is espeically appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive assumption\n",
    "\n",
    "The naive Bayes model assumes that given a class $G=j$, the features $X_k$ are independent:\n",
    "\n",
    "\\begin{equation}\n",
    "f_j(X) = \\prod_{k=1}^p f_{jk}(X_k)\n",
    "\\end{equation}\n",
    "\n",
    "While this assumption is generally not true, it does simplify the estimation dramatically:\n",
    "\n",
    "* The individual class-conditional marginal densities $f_{jk}$ can each be estimated separately using 1D kernel density estimates. This is in fact a generalization fo the original naive Bayes procedures, which used univariate Gaussians to represent these marginals.\n",
    "* If a component $X_j$ of $X$ is discrete, then an appropriate histogram estimate can be used. This provides a seamless wway of mixing variable types in a feature vector.\n",
    "\n",
    "Despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The reasons are related to FIGURE 6.15: Although the individual class density estimates may be biased, this bias might not hurt the posterior probabilities as much, especially near the decision regions. In fact, the problem may be able to withstand considerable bias for the savings in variance such a \"naive\" assumption earns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Starting from the independence assumption, we can derive the logit-transform (using class $J$ as the base):\n",
    "\n",
    "\\begin{align}\n",
    "\\log\\frac{\\text{Pr}(G=l|X)}{\\text{Pr}(G=J|X)} &= \\log\\frac{\\pi_l f_l(X)}{\\pi_J f_J(X)}\\\\\n",
    "&= \\log\\frac{\\pi_l \\prod_{k=1}^p f_{lk}(X_k)}{\\pi_J \\prod_{k=1}^p f_{Jk}(X_k)}\\\\\n",
    "&= \\log\\frac{\\pi_l}{\\pi_J} + \\sum_{k=1}^p \\log\\frac{f_{lk}(X_k)}{f_{Jk}(X_k)}\\\\\n",
    "&= \\alpha_l + \\sum_{k=1}^p g_{lk}(X_k).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity with generalized additive model\n",
    "\n",
    "This has the form of a _generalized additive model_ (Chapter 9). The models are fit in quite different ways though (Exercise 6.9). The relationship between naive Bayes and generalized additive models is analogous to that between LDA and logistic regression ($\\S$ 4.4.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.7. Radial Basis Functions and Kernels\n",
    "\n",
    "* functions can be represented as expansions in **basis functions** (Chapter 5):\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^M \\beta_j h_j(x).\n",
    "\\end{equation}\n",
    "\n",
    "* flexibility of the modeling depends on:\n",
    "  * picking an appropriate family of basis functions\n",
    "  * controlling the complexity of the representation by selection, regularization, or both\n",
    "\n",
    "* some of the families of basis functions have elements that are defined locally (e.g. B-splines)\n",
    "* if more flexibility desired in a particular region, then that region needs to be represented by more basis functions (more knots in the case of B-splines)\n",
    "* tensor products of $\\mathbb{R}$-local basis functions deliver basis functions local in $\\mathbb{R}^p$\n",
    "* even basis functions which are not local (e.g. truncated power bases for splines) can compose function $f(x)$ with local behavior - global effects are cancelled out\n",
    "\n",
    "* **kernel methods** achieve flexibility by fitting simple models in a region local to the target point $x_0$.\n",
    "* **radial basis functions** combine the kernel methods and the representation using basis functions by treating the kernel functions $K_\\lambda(\\xi,x)$ as basis functions\n",
    "\\begin{align}\n",
    "f(x) &= \\sum_{j=1}^M K_{\\lambda_i}(\\xi_j,x)\\beta_j \\\\\n",
    "&= \\sum_{j=1}^M D\\left( \\frac{\\|x-\\xi_j\\|}{\\lambda_j} \\right)\\beta_j\n",
    "\\end{align}\n",
    "\n",
    "  * each basis element is indexed by a location or _prototype_ parameter $xi_j$ and a scale parameter $\\lambda_j$\n",
    "  * popular choice for $D$ is the standard Gaussian density function\n",
    "  * several approaches to find the parameters $\\{\\lambda_j, \\xi_j, \\beta_j\\}$, for $j=1,\\cdots,M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "* the textbook uses Gaussian kernel and least squares methods\n",
    "\n",
    "A) Optimize the sum-of-squares with respect to all the parameters\n",
    "\\begin{equation}\n",
    "\\min_{\\{\\lambda_j,\\xi_j,\\beta_j\\}_1^M} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^M \\beta_j \\exp\\left( -\\frac{(x_i-\\xi_j)^T(x_i-\\xi_j)}{\\lambda_j^2} \\right)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "* the model is commonly referred to as an RBF network, an alternative to the sigmoidal neural network; the $\\xi_j$ and $\\lambda_j$ playing the role of the weights\n",
    "* the criterion is nonconvex with multiple local minima, and the algorithms for optimization are similar to those used for neural networks.\n",
    "\n",
    "B) Estimate the $\\{\\lambda_j,\\xi_j\\}$ separately from the $\\beta_j$\n",
    "* get $\\{\\lambda_j,\\xi_j\\}$, often chosen in an unsupervised way using the $X$ distribution alone\n",
    "  * e.g. fit a Gaussian mixture density model to the training $x_i$, which provides both the centers $\\xi_j$ and the scales $\\lambda_j$.\n",
    "  * other option mentioned: clustering methods\n",
    "* given $\\{\\lambda_j,\\xi_j\\}$, the estimation of $\\beta_j$ is a simple least squares problem \n",
    "\n",
    "C) Assume constant $\\lambda_j=\\lambda$\n",
    "* this creates *holes*-regions of $\\mathbb{R}^p$ where none of the kernels has appreciable support\n",
    "\n",
    "![title](imgs/6_7_holes_regions.png)\n",
    "\n",
    "* renormalization of the radial basis functions helps\n",
    "\n",
    "\\begin{equation}\n",
    "h_j(x) = \\frac{D(\\|x-\\xi_j\\|/\\lambda)}{\\sum_{k=1}^M D(\\|x-\\xi_k\\|/\\lambda)}\n",
    "\\end{equation}\n",
    "\n",
    "* e.g. Nadaraya-Watson kernel regression estimator in $\\mathbb{R}^p$\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= \\sum_{i=1}^N y_i \\frac{K_\\lambda(x_0,x_i)}{\\sum_{j=1}^N K_\\lambda(x_0,x_j)} \\\\\n",
    "&= \\sum_{i=1}^N y_i h_i(x_0),\n",
    "\\end{align}\n",
    "\n",
    "* with a basis function $h_i$ located at every observation and coefficients $y_i$:\n",
    "$\\xi_i=x_i$, $\\hat\\beta_i=y_i$, for $i=1,\\cdots,N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.8. Mixture Models for Density Estimation and Classification\n",
    "\n",
    "* **mixture model** is a useful tool for density estimation, and can be viewed as a kind of kernel method\n",
    "* Gaussian mixture model:\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{m=1}^M \\alpha_m\\phi(x;\\mu_m,\\mathbf{\\Sigma}_m)\n",
    "\\end{equation}\n",
    " \n",
    "  * mixing proportions $\\alpha_m$, $\\sum\\alpha_m=1$\n",
    "  * each Gaussian density has a mean $µ_m$ and covariance matrix $Σ_m$\n",
    "\n",
    "\n",
    "* fit usually by maximum likelihood, using the EM algorithm (Chapter 8)\n",
    "* probability that $x_i$ belongs to component $m$:\n",
    "\\begin{equation}\n",
    "\\hat{r}_{im} = \\frac{\\hat{\\alpha}_m\\phi(x_i;\\hat{\\mu}_m,\\hat{\\mathbf{\\Sigma}}_m)}{\\sum_{k=1}^M\\hat{\\alpha}_k\\phi(x_i;\\hat{\\mu}_k,\\hat{\\mathbf{\\Sigma}}_k)}\n",
    "\\end{equation}\n",
    "* on CHD data predicts comparably to linear logistic regression fit by MLE (error rate 32%)\n",
    "\n",
    "| Mixture model |  No | Yes |\n",
    "| ------------- | --- | --  |\n",
    "| **CHD No**    | 232 |  70 |\n",
    "| **CHD Yes**   | 76  |  84 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special cases\n",
    "\n",
    "* if covariance matrices constrained to be scalar: $\\mathbf{\\Sigma}_m=\\sigma_m\\mathbf{I}$, then the mixture model has the form of a radial basis expansion\n",
    "* if in addition $\\sigma_m = \\sigma >0$ and $M\\uparrow N$, then the maximum likelihood estimate approaches the kernel density estimate \n",
    "\n",
    "### Advanced applications\n",
    "* Using Bayes' theorem, separate mixture densities in each class lead to flexible models for $\\text{Pr}(G|X)$ (Chapter 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart disease example\n",
    "\n",
    "![title](imgs/CHD_Mixture.png) \n",
    "\n",
    "* histograms of Age for the no CHD and CHD groups separately, and combined\n",
    "* estimated densities from a Gaussian mixture model\n",
    "  * estimated component densities (blue and orange) \n",
    "  * estimated mixture density (green)\n",
    "* orange density has a very large standard deviation, and approximates a uniform density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.9. Computational Considerations\n",
    "\n",
    "* kernel, local regression and density estimation are **memory-based methods**: the model is the entire training data set, and the fitting is done at evaluation or prediction time\n",
    "* the computational cost to fit at a single obervation $x_0$ is $O(N)$ flops, except in oversimplified cases\n",
    "* the smoothing parameter(s) $\\lambda$ for kernel methods are typically determined off-line, at a cost of $O(N^2)$ flops\n",
    "* by comparison, an expansion in $M$ basis functions costs $O(M)$ for one evaluation, and typically $M\\sim O(\\log N)$\n",
    "* basis function methods have an initial cost of at least $O(NM^2+M^3)$\n",
    "*  triangulation schemes can reduce the computations (e.g. $\\textsf{loess}$ in S-Plus or R): compute the fit exactly at $M$ locations ($O(NM)$), and then use blending techniques to interpolate the fit elsewhere ($O(M)$ per evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Additional material\n",
    "## Local regression in $\\mathbb{R}^1$\n",
    "### Matrix formulation and equivalent kernel\n",
    "\n",
    "Define\n",
    "* the vector-valued function $b(x)^T = (1, x)$,\n",
    "* the $N\\times2$ regression matrix $\\mathbf{B}$ with $i$th row $b(x_i)^T$, and\n",
    "* the $N\\times N$ diagonal matrix $\\mathbf{W}(x_0)$ with $i$th diagonal element $K_\\lambda(x_0,x_i)$.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= b(x_0)^T \\left( \\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B} \\right)^{-1} \\mathbf{B}^T \\mathbf{W}(x_0) \\mathbf{y} \\\\\n",
    "&= \\sum_{i=1}^N l_i(x_0)y_i.\n",
    "\\end{align}\n",
    "\n",
    "Note that $l_i(x_0)$ do not involve $\\mathbf{y}$ and thus the estimate is _linear_ in $y_i$. These weights $l_i(x_0)$ combine the weighting kernel $K_\\lambda(x_0,\\cdot)$ and the least squares operations, and are sometimes referred to as the _equivalent kernel_.\n",
    "\n",
    "FIGURE 6.4 illustrates the effect of local linear regression on the equivalent kernel.\n",
    "\n",
    "![title](imgs/equivalent_kernel.png) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
