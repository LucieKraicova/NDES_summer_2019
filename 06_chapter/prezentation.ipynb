{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6. Kernel Smoothing Methods\n",
    "\n",
    "- **Local Regression** \n",
    "    - 6.1 Local Regression in $\\mathbb{R}^1$\n",
    "    - 6.3 Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.4 Structured Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.5 Local Likelyhood and Other Models\n",
    "- 6.6 **Kernel Density Estimation and Classification**\n",
    "- 6.7 Radial Basis Functions and Kernels\n",
    "- 6.8 Mixture Models for Density Estimation and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### What is a kernel? \n",
    "- weighting function $K_\\lambda(x_0,x_i)$, which assigns a weight to $x_i$ based on its distance from $x_0$.\n",
    "\n",
    "### How can we use kernels?\n",
    "- regression\n",
    "    - vizualization\n",
    "    - device for detecting nonlinearities\n",
    "    - localization device\n",
    "- classification\n",
    "- density estimation\n",
    "- ML \"kernel methods\" - kernel trick - regularized nonlinear modeling\n",
    "\n",
    "### What are advantages and disadvanteges? \n",
    "- memory-based - no training, all fitting during prediction \n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Local regression\n",
    "\n",
    "### How does local regression fit into the spectrum of regression methods?\n",
    "- **general regression setting**: \n",
    "    - estimating the regression function $f(X)$ over the domain $\\mathbb{R}^p$\n",
    "    - complexity restriction: \n",
    "        - define neighbourhood - explicitely controlled by widnow width and kernel type \n",
    "        - define behavior on that neighbourhood - constant, linear, polynomial \n",
    "- **model**: training data set, different at each point $x_0$.\n",
    "- **parameters** estimated from data: $\\lambda$\n",
    "- **estimated function** $\\hat{f}(X)$ is smooth in $\\mathbb{R}^p$\n",
    "- local regression estimate of $f(x_0)$ as $f_{\\hat{\\theta}}(x_0)$, where $\\hat{\\theta}$ minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of kernels\n",
    "\n",
    "### By type of window\n",
    "- **Metric window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D \\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "- **Adaptive window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{h_\\lambda(x_0)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "    - K nearest neighbours \n",
    "\\begin{equation}\n",
    "K_k(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{\\|x-x_k\\|} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Window width parameter:\n",
    "- the radius of the support region $\\lambda$\n",
    "- standard deviation for Gaussian kernel\n",
    "-  number $k$ of nearest neighbors in $k$-nearest neighborhoods\n",
    "\n",
    "## By functional form\n",
    "- **Compact support**\n",
    "- Epanechnikov\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac34 (1-t^2) & \\text{if } |t| \\le 1; \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    " - Tri-cube\n",
    "    \n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{70}{81}(1-|t|^3)^3 & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "- Uniform\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{1}{2} & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- **Infinite support** \n",
    "    - Gaussian \n",
    "\\begin{equation}    \n",
    "K_\\lambda(x_0,x) = \\frac{1}{\\lambda}\\exp\\left(-\\frac{\\|x-x_0\\|^2}{2\\lambda}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/kernels_textbook.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the polynomial order\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "K nearest neighbours\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\text{Ave}\\left( y_i|x_i\\in N_k(x_0) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Nadarayea-Watson weighted average\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Local linear regression\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\beta(x_0)x_i \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Local polynomial regression\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta_j(x_0), j=1,\\cdots,d} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\sum_{j=1}^d \\beta_j(x_0)x_i^j \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\sum_{j=1}^N \\hat\\beta(x_0)x_o^j.\n",
    "\\end{equation}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Bias-variance tradeoff\n",
    "\\begin{align}\n",
    "\\text{E}\\hat{f}(x_0) &= \\sum_{i=1}^N l_i(x_0)f(x_i) \\\\\n",
    "&= f(x_0)\\sum_{i=1}^N l_i(x_0) + f'(x_0)\\sum_{i=1}^N (x_i-x_0)l_i(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R,\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat{f}(x_0)) = \\sigma^2 \\|l(x_0)\\|^2,\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/constant_to_linear.png) \n",
    "![title](imgs/linear_to_quadratic.png) \n",
    "![title](imgs/variance_and_polynomial_order.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting window width\n",
    "\n",
    "Bias-variance tradeoff: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# K-nearest neighbors (Section 2.3)\n",
    "def knn(k: int, point:float,\n",
    "        data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    idx_sorted = scipy.argsort((data_x-point)*(data_x-point))[:k]\n",
    "    return data_y[idx_sorted].mean()\n",
    "\n",
    "# Constant fit with Epanechnikov\n",
    "def epanechnikov(lmbda:float, point:float,\n",
    "                 data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    t = scipy.absolute(data_x-point)/lmbda  # argment for D\n",
    "    k = scipy.where(t <= 1, 0.75*(1-t), 0)\n",
    "    return (k @ data_y).sum()/k.sum()\n",
    "\n",
    "\n",
    "# Constant fit with gaussian\n",
    "def gaussian(lmbda:float, point:float,\n",
    "                 data_x:scipy.ndarray, data_y:scipy.ndarray) -> float:\n",
    "    k = (1/lmbda) * scipy.exp(\n",
    "        -(scipy.absolute(data_x-point)*scipy.absolute(data_x-point)/(2*lmbda))) \n",
    "    return (k @ data_y).sum()/k.sum()\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# x grid, true y\n",
    "xgrid = scipy.linspace(0, 1, 1001)\n",
    "y_true = scipy.sin(4*xgrid)\n",
    "\n",
    "# Sample\n",
    "size_sample = 100\n",
    "x_sample = scipy.random.uniform(size=size_sample)\n",
    "y_sample = scipy.sin(4*x_sample) + scipy.randn(size_sample)/3\n",
    "\n",
    "\n",
    "def set_ax(ax, y_estimated, mask, title:str, idx_x0=500, show_true_y=True):\n",
    "    x0 = xgrid[idx_x0]  # .5\n",
    "    if show_true_y:\n",
    "        ax.plot(xgrid, y_true, color='C0', linewidth=2)\n",
    "    ax.plot(xgrid, y_estimated, color='C2')\n",
    "    ax.plot(x_sample[mask], y_sample[mask],'o', color='C3', mfc='none')\n",
    "    ax.plot(x_sample[~mask], y_sample[~mask],'o', color='C0', mfc='none')\n",
    "    ax.plot((x0, x0), (ax.get_ylim()[0], y_estimated[idx_x0]), 'o-', color='C3')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "def plot_two(k,lmbda,idx_x0=500, show_true_y=True):\n",
    "    fig61 = plt.figure(61, figsize=(18, 10))\n",
    "    x0 = xgrid[idx_x0]  # .5\n",
    "    ax1=fig61.add_subplot(1, 2, 1)\n",
    "    ax2=fig61.add_subplot(1, 2, 2)\n",
    "\n",
    "    # K nearest neighbours\n",
    "    \"\"\"\n",
    "    x_k = x_sample[scipy.argsort((x_sample-x0)*(x_sample-x0))[k-1:k]][0]\n",
    "    mask = scipy.absolute(x_sample-x0) <= scipy.absolute(x0-x_k)\n",
    "    set_ax(ax=ax1, \n",
    "           y_estimated=scipy.array([knn(k, x, x_sample, y_sample) for x in xgrid]), \n",
    "           mask=mask, \n",
    "           title='Nearest-Neighbor Kernel',\n",
    "           idx_x0=idx_x0, \n",
    "           show_true_y=show_true_y)\n",
    "    \"\"\"\n",
    "    # Epanechnikov\n",
    "    set_ax(ax=ax1, \n",
    "           y_estimated = scipy.array([epanechnikov(lmbda, x, x_sample, y_sample)\n",
    "                                  for x in xgrid]), \n",
    "           mask = scipy.absolute(x_sample-x0)/lmbda <= 1, \n",
    "           title='Epanechnikov Kernel',\n",
    "           idx_x0=idx_x0,\n",
    "           show_true_y=show_true_y) \n",
    "    \n",
    "    # Gaussian \n",
    "    set_ax(ax=ax2, \n",
    "           y_estimated = scipy.array([gaussian(lmbda, x, x_sample, y_sample)\n",
    "                                  for x in xgrid]), \n",
    "           mask = np.array([True]*len(x_sample)),\n",
    "           title='Gaussian Kernel',\n",
    "           idx_x0=idx_x0,\n",
    "           show_true_y=show_true_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2f6efa60ca485294310e971f6e30f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='k', min=1), FloatSlider(value=0.51, description='lmbda'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_two(k, lmbda, idx_x0=500, show_true_y=True)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_two, \n",
    "         k=widgets.IntSlider(min=1,max=100,step=1,value=10),\n",
    "         lmbda=(0.02,1.0,0.01),\n",
    "         idx_x0=widgets.IntSlider(min=1,max=1000,step=1,value=500),\n",
    "         show_true_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.3. Local Regression in $\\mathbb{R}^p$\n",
    "### Generalizing previous notation\n",
    "\n",
    "Let $b(X)$ be a vector of polynomial terms in $X$ of maximum degree $d$; e.g., with $d=1$ and $p=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2);\n",
    "\\end{equation}\n",
    "\n",
    "with $d=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2);\n",
    "\\end{equation}\n",
    "\n",
    "and trivially with $d=0$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "At each $x_0 \\in \\mathbb{R}^p$ solve\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - b(x_i)^T\\beta(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "to produce the fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = b(x_0)^T \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Typically the kernel will be a radial function, such as the radial Epanechnikov or tri-cube kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|\\cdot\\|$ is the Euclidean norm.\n",
    "\n",
    "Since the Euclidean norm depends on the units in each coordinate, it makes most sense to standardize each predictor, e.g., to unit standard deviation, prior to smoothing.\n",
    "\n",
    "### Problems with high dimension \n",
    "\n",
    "- **Curse of dimensionality** \n",
    "    - impossible to simultaneously maintain localness ($\\Rightarrow$ low bias) and a sizeable sample in the neighborhood ($\\Rightarrow$ low variance) as the dimension increases, without the total sample size increasing exponentially in $p$.\n",
    "    - correcting by local or higher polynomial fit is useful\n",
    "- **Vizualization problematic**\n",
    "\n",
    "\n",
    "- Solution: introduce structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.4. Structured Local Regression Models in $\\mathbb{R}^p$\n",
    "\n",
    "> When the dimension to sample-size ratio is unfavorable, local regression does not help us much, unless we are willing to make some structural assumptions about the model.\n",
    ">\n",
    "> Much of this book is about structured regression and classification models. Here we focus on some approaches directly related to kernel methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.1. Structured Kernels\n",
    "### Standardization\n",
    "\n",
    "One line of approach is to modify the kernel. The default spherical kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "gives equal weight to each coordinate, and so a natural default strategy is to standardize each variable to unit standard deviation.\n",
    "\n",
    "A more general approach is to use a positive semidefinite matrix $\\mathbf{A}$ to weigh the different coordinates:\n",
    "\n",
    "\\begin{equation}\n",
    "K_{\\lambda,\\mathbf{A}}(x_0,x) = D \\left( \\frac{(x-x_0)^T\\mathbf{A}(x-x_0)}\\lambda \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on $\\mathbf{A}$. For example, if $\\mathbf{A}$ is diagonal, then we can increase or decrease the influence of individual predictors $X_j$ by increasing or decreasing $A_{jj}$.\n",
    "\n",
    "Often the predictors are many and highly correlated, such as those arising from digitized analog signals or images. The covariance function of the predictors can be used to tailor a metric $\\mathbf{A}$ that focuses less, say, on high-freqeuncy contrast (Exercise 6.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.2. Structured Regression Functions\n",
    "\n",
    "We are trying to fit a regression function\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}(Y|X) = f(X_1,X_2,\\cdots,X_p)\n",
    "\\end{equation}\n",
    "\n",
    "in $\\mathbb{R}^p$, in which every level of interaction is potentially present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure via ANOVA decomposition\n",
    "It is natural to consider ANOVA decompositions of the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(X_1,X_2,\\cdots,X_p) = \\alpha + \\sum_j g_j(X_j) + \\sum_{k<l} g_{kl}(X_k,X_l) + \\cdots\n",
    "\\end{equation}\n",
    "\n",
    "and then introduce structure by eliminating some of the higher-order terms.\n",
    "\n",
    "Additive models assume only main effect terms:\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha + \\sum_{j=1}^p g_j(X_j);\n",
    "\\end{equation}\n",
    "\n",
    "Second-order models will have terms with interactions of order at most two, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfitting\n",
    "In Chapter 9, we describe iterative _backfitting_ algorithms for fitting such lower-order interaction models. In the additive model, for example, if all but the $k$th term is assumed known, then we can estimate $g_k$ by local regression of $Y - \\sum_{j\\neq k}g_j(X_j)$ on $X_k$. This is done for each function in turn, repeatedly, until convergence. The important detail is that at any stage, 1D local regression is all that is needed. The same ideas can be used to fit low-dimensional ANOVA decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying coefficients models\n",
    "An important special case of these structured models are the class ofr _varying coefficient models_. Suppose that we divide the $p$ predictors in $X$ into a set $(X_1, X_2, \\cdots, X_q)$ with $q<p$, and the remainder of the variables we collect in the vector $Z$. We then assume the conditionally linear model\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha(Z) + \\beta_1(Z)X_1 + \\cdots + \\beta_q(Z)X_q.\n",
    "\\end{equation}\n",
    "\n",
    "For given $Z$, this is a linear model, but each of the coefficients can vary with $Z$. It is natural to fit such a model by locally weighted least squares:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(z_0),\\beta(z_0)} \\sum_{i=1}^N K_\\lambda(z_0,z_i) \\left( y_i - \\alpha(z_0) - x_{i1}\\beta_1(z_0) - \\cdots - x_{qi}\\beta_q(z_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 6.10 illustrates the idea on measurements of the human aorta. A longstanding claim has been that the aorta thickens with $\\textsf{age}$. Here we model the $\\textsf{diameter}$ of the aorta as a linear function of $\\textsf{age}$, but allow the coefficients to vary with $\\textsf{gender}$ and $\\textsf{depth}$ down the aorta. We used a local regression model separately for males and females. Wile aorta clearly does thicken with age at the higher regions of the aorta, the relationship fades with distance down the aorta. FIGURE 6.11 shows the intercept and slope as a function of depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.5. Local Likelihood and Other Models\n",
    "\n",
    "The concept of local regression and varying coefficient models is extremely broad: Any parametric model can be made local if the fitting method accommodates observation weights. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each observation $y_i$ is a parameter\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i = \\theta(x_i) = x_i^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "linear in the covariate(s) $x_i$, and inference for $\\beta$ is based on the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta) = \\sum_{i=1}^N l(y_i,x_i^T\\beta).\n",
    "\\end{equation}\n",
    "\n",
    "We can model $\\theta(X)$ more flexibly by using the likelihood local to $x_0$ for inference of $\\theta(x_0) = x_0^T\\beta(x_0)$:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta(x_0)) = \\sum_{i=1}^N K_\\lambda(x_0,x_i) l(y_i,x_i^T\\beta(x_0)).\n",
    "\\end{equation}\n",
    "\n",
    "Many likelihood models, in particular the family of  generalized linear models including logistic and log-linear models, involve the covariates in a linear fashion. Local likelihood allows a relaxation from a globally linear model to one that is locally linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of local likelihood, we consider the local version of the multiclass linear logistic regression model of Chapter 4. The data consist of features $x_i$ and an associated categorical response $g_i \\in \\{1,2,\\cdots,J\\}$, and the linear model has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=j|X=x) = \\frac{\\exp\\left(\\beta_{j0} + \\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\beta_{k0} + \\beta_k^Tx\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "The local log-likelihood for this $J$ class model can be written\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left\\{ \\beta_{g_i 0}(x_0) + \\beta_{g_i}^T(x_i-x_0) - \\log\\left( 1 + \\sum_{k=1}^{J-1} \\exp\\left( \\beta_{k0}(x_0) + \\beta_k(x_0)^T(x_i-x_0) \\right) \\right) \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Notice that\n",
    "* we have used $g_i$ as a subscript in the first line to pick out the appropriate numerator;\n",
    "* $\\beta_{J0} = 0$ and $\\beta_J = 0$ by the definition of the model;\n",
    "* we have centered the local regressions at $x_0$, so that the fitted posterior probabilities at $x_0$ are simply  \n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat{\\text{Pr}}(G=j|X=x) = \\frac{\\exp\\left(\\hat\\beta_{j0} + \\hat\\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\hat\\beta_{k0} + \\hat\\beta_k^Tx\\right)}.\n",
    "  \\end{equation}\n",
    "  \n",
    "This model can be used for flexible multiclass classification in moderately low dimensions, although successes have been reported with the high-dimensional ZIP-code classficiation problem. Generalized additive models (Chapter 9) using kernel smoothing methods are closely related, and avoid dimensionality problems by assuming an additive structure for the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.6. Kernel Density Estimation and Classification\n",
    "\n",
    "Kernel density estimation is an unsupervised learning procedure, which historically precredes kernel regression. It also leads to naturally to a simple family of procedures for nonparametric classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.1. Kernel Density Estimation\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "> Suppose we have a random sample $x_1,\\cdots,x_N$ drawn from a probability density $f_X(x)$, and we wish to estimate $f_X$ at a point $x_0$.\n",
    "\n",
    "For simplicity assume for now that $X\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local estimate\n",
    "\n",
    "A natural local estimate has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_X(x_0) = \\frac{\\#x_i \\in \\mathcal{N}(x_0)}{N\\lambda},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{N}$ is a small metric neighborhood around $x_0$ of width $\\lambda$.\n",
    "\n",
    "This estimate is bumpy, so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate with kernels\n",
    "\n",
    "the smooth _Parzen_ estimate is preferred.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N\\lambda} \\sum_{i=1}^N K_\\lambda(x_0,x_i),\n",
    "\\end{equation}\n",
    "\n",
    "because it counts observations close to $x_0$ with weights that decrease with distance from $x_0$. In this case a popular choice for $K_\\lambda$ is the Gaussian kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = \\phi\\left(\\frac{|x-x_0|}\\lambda\\right).\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 6.13 shows a Gaussian kernel density fit to the sample values for $\\textsf{systolic blood pressure}$ for the $\\textsf{CHD}$ group.\n",
    "\n",
    "Letting $\\phi_\\lambda$ denote the Gaussian density with mean zero and standard-deviation $\\lambda$, the Parzen estimate has the form\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}_X(x) &= \\frac1N \\sum_{i=1}^N \\phi_\\lambda(x-x_i) \\\\\n",
    "&= \\left( \\hat{F}\\star\\phi_\\lambda \\right)(x),\n",
    "\\end{align}\n",
    "\n",
    "the convolution of the sample empirical distribution $\\hat{F}$ with $\\phi_\\lambda$. The distribution $\\hat{F}(x)$ puts mass $1/N$ at each of the observed $x_i$, and is jumpy; in $\\hat{f}_X(x)$ we have smoothed $\\hat{F}$ by adding independent Gaussian noise to each observation $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution = local averaging\n",
    "\n",
    "The Parzen density estimate is the equivalent of the local average, and improvements have been proposed along the lines of local regression [on the log scale for densities; see Loader (1999)]. We will not pursue these here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In $\\mathbb{R}^p$\n",
    "\n",
    "The natural generalization of the Gaussian density estimate amounts to using the Gaussian product kernel,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N(2\\lambda^2\\pi)^{\\frac{p}2}} \\sum_{i=1}^N \\exp \\left\\{ -\\frac12 \\left( \\|x_i-x_0\\|/\\lambda \\right)^2 \\right\\}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.2. Kernel Density Classification\n",
    "\n",
    "One can use nonparametric density estimates for classification in a straight-forward fashion using Bayes' theorem.\n",
    "\n",
    "Suppose for a $J$ class problem\n",
    "* we fit nonparametric density estimates $\\hat{f}_j(X)$, for $j=1,\\cdots,J$ separately in each of the classes,\n",
    "* and we also have estimates of the class priors $\\hat\\pi_j$ (usually the sample proportions).\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{\\hat\\pi_j \\hat{f}_j(x_0)}{\\sum_{k=1}^J \\hat\\pi_k \\hat{f}_k(x_0)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty with sparse data\n",
    "\n",
    "FIGURE 6.14 uses this method to estimate to prevalence of $\\textsf{CHD}$ for the heart risk factor study, and should be compared with the left panel of FIGURE 6.12. The main difference occurs in the region of high SBP in the right panel of FIGURE 6.14. In this region the data are sparse for both classes, and since the Gaussian kernel density estimates use metric kernels, the density estimates are low and of poor quality (high variance) in these region.\n",
    "\n",
    "The local logistic regression method uses the tri-cube kernel with $k$-NN of the local linear assumption to smooth out the estimate (on the logit scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If classification is the ultimate goal, then learning the separate class densities well may be unnecessary, and can in fact be misleading.\n",
    "\n",
    "FIGURE 6.15 shows an example where the densities are both multimodal, but the posterior ratio is quite smooth. In learning the separate densities from data, one might decide to settle for a rougher, high-variance fit to capture these features, which are irrelevant for the purposes of estimating the posterior probabilities. In fact, if classification is the ultimate goal, then we need only to estimate the posterior well near the decision boundary. e.g., for two classes, this is the set\n",
    "\n",
    "\\begin{equation}\n",
    "\\{ x| \\text{Pr}(G=1|X=x) = \\frac12 \\}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.3. The Naive Bayes Classifier\n",
    "\n",
    "This is a technique that has remained popular over the years, despite its name (a.k.a. \"Idiot's Bayes\"). It is espeically appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive assumption\n",
    "\n",
    "The naive Bayes model assumes that given a class $G=j$, the features $X_k$ are independent:\n",
    "\n",
    "\\begin{equation}\n",
    "f_j(X) = \\prod_{k=1}^p f_{jk}(X_k)\n",
    "\\end{equation}\n",
    "\n",
    "While this assumption is generally not true, it does simplify the estimation dramatically:\n",
    "\n",
    "* The individual class-conditional marginal densities $f_{jk}$ can each be estimated separately using 1D kernel density estimates. This is in fact a generalization fo the original naive Bayes procedures, which used univariate Gaussians to represent these marginals.\n",
    "* If a component $X_j$ of $X$ is discrete, then an appropriate histogram estimate can be used. This provides a seamless wway of mixing variable types in a feature vector.\n",
    "\n",
    "Despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The reasons are related to FIGURE 6.15: Although the individual class density estimates may be biased, this bias might not hurt the posterior probabilities as much, especially near the decision regions. In fact, the problem may be able to withstand considerable bias for the savings in variance such a \"naive\" assumption earns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Starting from the independence assumption, we can derive the logit-transform (using class $J$ as the base):\n",
    "\n",
    "\\begin{align}\n",
    "\\log\\frac{\\text{Pr}(G=l|X)}{\\text{Pr}(G=J|X)} &= \\log\\frac{\\pi_l f_l(X)}{\\pi_J f_J(X)}\\\\\n",
    "&= \\log\\frac{\\pi_l \\prod_{k=1}^p f_{lk}(X_k)}{\\pi_J \\prod_{k=1}^p f_{Jk}(X_k)}\\\\\n",
    "&= \\log\\frac{\\pi_l}{\\pi_J} + \\sum_{k=1}^p \\log\\frac{f_{lk}(X_k)}{f_{Jk}(X_k)}\\\\\n",
    "&= \\alpha_l + \\sum_{k=1}^p g_{lk}(X_k).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity with generalized additive model\n",
    "\n",
    "This has the form of a _generalized additive model_ (Chapter 9). The models are fit in quite different ways though (Exercise 6.9). The relationship between naive Bayes and generalized additive models is analogous to that between LDA and logistic regression ($\\S$ 4.4.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.7. Radial Basis Functions and Kernels\n",
    "### Review on basis expansion\n",
    "\n",
    "In Chapter 5, functions are represented as expansions in basis functions:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^M \\beta_j h_j(x).\n",
    "\\end{equation}\n",
    "\n",
    "The art of flexible modeling using basis expansions consists of picking an appropriate family of basis functions, and then controlling the complexity of the representation by selection, regularization, or both.\n",
    "\n",
    "Some of the families of basis functions have elements that are defined locally; e.g., B-splines. If more flexibility is desired in a particular region, then that region needs to be represented by more basis functions (which in the case of B-splines translates to more knots).\n",
    "\n",
    "Tensor products of $\\mathbb{R}$-local basis functions deliver basis functions local in $\\mathbb{R}^p$. Not all basis functions are local -- e.g., the truncated power bases for splines, or  sigmoidal basis functions $\\sigma(\\alpha_0 + \\alpha x)$ used in neural-networks (Chapter 11).\n",
    "\n",
    "The composed function $f(x)$ can nevertheless show local behavior, because of the particular signs and values of the coefficients causing cancellations of global effects. For example, the truncated power basis has an equivalent B-spline basis for the same space of functions; the cancellation is exact in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review on kernel smoothing\n",
    "\n",
    "Kernel methods achieve flexibility by fitting simple models in a region local to the target point $x_0$. Localization is achieved via a weighting kernel $K_\\lambda$, and individual observations receive weights $K_\\lambda(x_0,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis functions\n",
    "\n",
    "Radial basis functions combine these ideas, by treating the kernel functions $K_\\lambda(\\xi,x)$ as basis functions. This leads to the model\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= \\sum_{j=1}^M K_{\\lambda_i}(\\xi_j,x)\\beta_j \\\\\n",
    "&= \\sum_{j=1}^N D\\left( \\frac{\\|x-\\xi_j\\|}{\\lambda_j} \\right)\\beta_j,\n",
    "\\end{align}\n",
    "\n",
    "where each basis element is indexed by a location or _prototype_ parameter $xi_j$ and a scale parameter $\\lambda_j$. A popular choice for $D$ is the standard Gaussian density function.\n",
    "\n",
    "There are several approaches to learning the parameters $\\{\\lambda_j, \\xi_j, \\beta_j\\}$, for $j=1,\\cdots,M$. For simplicity we will focus on least squares methods for regression, and use the Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  \n",
    "Optimize the sum-of-squares w.r.t. all the parameters,\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\{\\lambda_j,\\xi_j,\\beta_j\\}_1^M} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^M \\beta_j \\exp\\left( -\\frac{(x_i-\\xi_j)^T(x_i-\\xi_j)}{\\lambda_j^2} \\right)\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This model is commonly referred to as an RBF network, an alternative to the sigmoidal neural network; the $\\xi_j$ and $\\lambda_j$ playing the role of the weights. This criterion is nonconvex with multiple local minima, and the algorithms for optimization are similar to those used for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  \n",
    "Estimate the $\\{\\lambda_j,\\xi_j\\}$ separately from the $\\beta_j$. Given $\\{\\lambda_j,\\xi_j\\}$, the estimation of $\\beta_j$ is a simple least squares problem.\n",
    "\n",
    "Often the kernel parameters $\\lambda_j$ and $\\xi_j$ are chosen in an unsupervised way using the $X$ distribution alone. One of the methods is to fit a Gaussian mixture density model to the training $x_i$, which provides both the centers $\\xi_j$ and the scales $\\lambda_j$.\n",
    "\n",
    "Other even more adhoc approaches use clustering methods to local the prototypes $\\xi_j$, and treat $\\lambda_j = \\lambda$ as a hyper-parameter. The obvious drawback of these approaches is that the conditional distribution $\\text{Pr}(Y|X)$ and in particular $\\text{E}(Y|X)$ is having no say in where the action is concentrated.\n",
    "\n",
    "On the positive side, they are much simpler to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renormalize\n",
    "\n",
    "While it would seem attractive to reduce the parameter set and assume a constant value for $\\lambda_j=\\lambda$, this can have an undesirable side effect of creating _holes_ -- regions of $\\mathbb{R}^p$ where none of the kernels has appreciable support, as illustrated in FIGURE 6.16 upper panel. _Renormalized_ radial basis functions,\n",
    "\n",
    "\\begin{equation}\n",
    "h_j(x) = \\frac{D(\\|x-\\xi_j\\|/\\lambda)}{\\sum_{k=1}^M D(\\|x-\\xi_k\\|/\\lambda)}\n",
    "\\end{equation}\n",
    "\n",
    "avoid this problem (FIGURE 6.16 lower panel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nadaraya-Watson kernel regression estimator in $\\mathbb{R}^p$ can be viewed as an expansion in renormalized radial basis functions,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= \\sum_{i=1}^N y_i \\frac{K_\\lambda(x_0,x_i)}{\\sum_{j=1}^N K_\\lambda(x_0,x_j)} \\\\\n",
    "&= \\sum_{i=1}^N y_i h_i(x_0),\n",
    "\\end{align}\n",
    "\n",
    "with a basis function $h_i$ located at every observation and coefficients $y_i$; i.e.,\n",
    "\n",
    "\\begin{align}\n",
    "\\xi_i &= x_i, \\\\\n",
    "\\hat\\beta_i &= y_i, \\text{ for } i=1,\\cdots,N.\n",
    "\\end{align}\n",
    "\n",
    "Note the similarity between the expansion and the solution (5.50 on page 169) to the regularization problem induced by the kernel $K$. Radial basis functions form the bridge between the modern \"kernel methods\" and local fitting technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.8. Mixture Models for Density Estimation and Classification\n",
    "\n",
    "The mixture model is a useful tool for density estimation, and can be viewed as a kind of kernel method. The  Gaussian mixture model has the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{m=1}^M \\alpha_m\\phi(x;\\mu_m,\\mathbf{\\Sigma}_m)\n",
    "\\end{equation}\n",
    "\n",
    "with mixing proportions $\\sum\\alpha_m=1$.\n",
    "\n",
    "In general, mixture models can use any component densities in place of the Gaussian: The Gaussian mixture model is by far the most popular.\n",
    "\n",
    "The parameters are usually fit by maximum likelihood, using the EM algorithm as described in Chapter 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some special cases arise\n",
    "\n",
    "* If the covariance matrices are constrained to be scalar: $\\mathbf{\\Sigma}_m=\\sigma_m\\mathbf{I}$, then it has the form of a radial basis expansion.\n",
    "* If in addition $\\sigma_m = \\sigma >0$ is fixed, and $M\\uparrow N$, then the maximum likelihood estimate approaches the kernel density estimate  \n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat{f}_X(x_0) = \\frac1{N\\lambda} \\sum_{i=1}^N K_\\lambda(x_0,x)\n",
    "  \\end{equation}\n",
    "  \n",
    "  where $\\hat\\alpha_m = 1/N$ and $\\hat\\mu_m = x_m$.\n",
    "\n",
    "Using Bayes' theorem, separate mixture densities in each class lead to flexible models for $\\text{Pr}(G|X)$ (Chapter 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart disease, again\n",
    "\n",
    "FIGURE 6.17 shows an application of mixtures to the heart disease risk factor study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.9. Computational Considerations\n",
    "\n",
    "Kernel and local regression and density estimation are _memory-based_ methods: The model is the entire training data set, and the fitting is done at evaluation or prediction time. For many real-time applications, this can make this class of methods infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational cost to fit at a single obervation $x_0$ is $O(N)$ flops, except in oversimplified cases (such as square kernels). By comparison, an expansion in $M$ basis functions costs $O(M)$ for one evaluation, and typically $M\\sim O(\\log N)$. Basis function methods have an initial cost of at least $O(NM^2+M^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothing parameter(s) $\\lambda$ for kernel methods are typically determined off-line, e.g. using cross-validation, at a cost of $O(N^2)$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular implementations of local regression, such as the $\\textsf{loess}$ function in S-PLUS and R, and the $\\textsf{locfit}$ procedure (Loader, 1999), use triangulation schemes to reduce the computations. They compute the fit exactly at $M$ carefully chosen locations ($O(NM)$), and then use blending techniques to interpolate the fit elsewhere ($O(M)$ per evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Additional material\n",
    "## Local regression in $\\mathbb{R}^1$\n",
    "### Matrix formulation and equivalent kernel\n",
    "\n",
    "Define\n",
    "* the vector-valued function $b(x)^T = (1, x)$,\n",
    "* the $N\\times2$ regression matrix $\\mathbf{B}$ with $i$th row $b(x_i)^T$, and\n",
    "* the $N\\times N$ diagonal matrix $\\mathbf{W}(x_0)$ with $i$th diagonal element $K_\\lambda(x_0,x_i)$.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= b(x_0)^T \\left( \\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B} \\right)^{-1} \\mathbf{B}^T \\mathbf{W}(x_0) \\mathbf{y} \\\\\n",
    "&= \\sum_{i=1}^N l_i(x_0)y_i.\n",
    "\\end{align}\n",
    "\n",
    "Note that $l_i(x_0)$ do not involve $\\mathbf{y}$ and thus the estimate is _linear_ in $y_i$. These weights $l_i(x_0)$ combine the weighting kernel $K_\\lambda(x_0,\\cdot)$ and the least squares operations, and are sometimes referred to as the _equivalent kernel_.\n",
    "\n",
    "FIGURE 6.4 illustrates the effect of local linear regression on the equivalent kernel.\n",
    "\n",
    "![title](imgs/equivalent_kernel.png) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
